{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11accee7",
   "metadata": {},
   "source": [
    "\n",
    "# Semantische Suche in arXiv-Artikeln mit LangChain\n",
    "\n",
    "In dieser Ãœbung verwenden wir arXiv, um echte wissenschaftliche Abstracts abzurufen, sie zu chunken und in einer Vektordatenbank zu speichern; und schlieÃŸlich darauf semantische Suchen durchzufÃ¼hren.\n",
    "\n",
    "Dabei lernen Sie: \n",
    "- arXiv-Abstracts abzurufen und zu verarbeiten  \n",
    "- Texte in Chunks aufzteilen  \n",
    "- Embeddings zu erzeugen   \n",
    "- eine Vektordatenbank aufbauen (Chroma)  \n",
    "- semantische Ã„hnlichkeitssuche durchzufÃ¼hren  \n",
    "\n",
    "\n",
    "## arXiv\n",
    "arXiv (ausgesprochen **archive**) ist eine Plattform fÃ¼r wissenschaftliche Preprints.\n",
    "\n",
    "Forschende aus Bereichen wie Physik, Informatik, Mathematik, KI oder Statistik verÃ¶ffentlichen dort ihre Arbeiten, bevor sie in Fachzeitschriften erscheinen.\n",
    "\n",
    "- offen, kostenlos und ohne Login nutzbar.\n",
    "- strukturierte Metadaten (Titel, Abstract, Autor\\*innen, VerÃ¶ffentlichungsdatum)\n",
    "- API-Zugang, dadurch nutzbar fÃ¼r Text- und Datenanalysen\n",
    "\n",
    "\n",
    "## LangChain\n",
    "LangChain ist ein Python-Framework zur Entwicklung von Anwendungen, die mit groÃŸen Sprachmodellen (LLMs) oder semantischer Textverarbeitung arbeiten.\n",
    "Es stellt Bausteine bereit fÃ¼r:\n",
    "\n",
    "- Datenverarbeitung: Laden, Aufteilen und Strukturieren von Texten\n",
    "- Embeddings: Umwandlung von Text in numerische Vektoren zur semantischen Analyse\n",
    "- Vektordatenbanken: Speicherung und Wiederfinden semantisch Ã¤hnlicher Inhalte\n",
    "- (optional) LLMs: Anbindung von Modellen wie GPT, Mistral oder Claude fÃ¼r Textgenerierung und Dialoge\n",
    "\n",
    "In dieser Ãœbung konzentrieren wir uns ausschlieÃŸlich auf den Retrieval-Teil â€“ also das Chunken, Erstellen von Embeddings und semantische Suchen, nicht auf die Textgenerierung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2762641",
   "metadata": {},
   "source": [
    "## 0. Installation benÃ¶tigter Bibliotheken\n",
    "\n",
    "- Aktivieren Sie Ihre `venv`\n",
    "- Aktivieren Sie die benÃ¶tigten Bibliotheken: `pip install -U langchain langchain-community langchain-text-splitters langchain-huggingface chromadb sentence-transformers torch arxiv tqdm`\n",
    "- Aktualisieren Sie Ihre `requirements.txt`\n",
    "- Nach der Installation mÃ¼ssen Sie den Kernel des Notebooks ggf. neu starten, damit die neuen Bibliotheken verfÃ¼gbar sind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc1547",
   "metadata": {},
   "source": [
    "## 1. Data Collection mit der arXiv API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b0323",
   "metadata": {},
   "source": [
    "1. Ãœberlegen Sie sich einen Suchbegriff und definieren Sie eine entsprechende Variable. \n",
    "2. Ãœberlegen Sie, wieviele Treffer Sie erhalten mÃ¶chten und definieren Sie eine entsprechende Variable. \n",
    "3. Ãœbergeben Sie die beiden Variablen als Parameter `query` und `max_results` an das untenstehende Search()-Objekt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d003347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from tqdm import tqdm\n",
    "\n",
    "query = \"RAG\"\n",
    "max_results = 50\n",
    "search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38131899",
   "metadata": {},
   "source": [
    "4. FÃ¼hren Sie die Suche aus und lassen Sie sich die Trefferliste mit dem vorhandenen Code ausgeben. \n",
    "5. Bonus: Ã„ndern Sie die Sortierung der Ergebnisse von `Relevance` auf `SubmittedDate` und fÃ¼hren Sie die Suche erneut aus.\n",
    "- `SubmittedDate`sortiert die Ergebnisse nach Einreichungsdatum &rarr; neueste zuerst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21289c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2_/sb5q2w3x6zz5s2m0zwsdhy_c0000gn/T/ipykernel_96520/959668180.py:2: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Artikel geladen.\n",
      "\n",
      "ðŸ“„ MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\n",
      "Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a ...\n",
      "\n",
      "ðŸ“„ TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages\n",
      "Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented ...\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for result in search.results():\n",
    "    docs.append({\n",
    "        \"title\": result.title,\n",
    "        \"summary\": result.summary,\n",
    "        \"url\": result.entry_id,\n",
    "        \"published\": result.published\n",
    "    })\n",
    "\n",
    "print(f\"{len(docs)} Artikel geladen.\")\n",
    "for d in docs[:2]:\n",
    "    print(f\"\\nðŸ“„ {d['title']}\\n{d['summary'][:250]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73ec62",
   "metadata": {},
   "source": [
    "## 2. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeab3e4",
   "metadata": {},
   "source": [
    "1. Betrachten Sie den folgenden Code: Welche Chunking-Methode wird hier eingesetzt? \n",
    "- **Fixed-size Chunking**\n",
    "2. Passen Sie den Code so an, dass Sliding Window berÃ¼cksichtigt wird. \n",
    "3. Experimentieren Sie mit verschiedenen Werten fÃ¼r die GrÃ¶ÃŸe der Chunks und des Sliding Window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204945f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 Chunks erzeugt.\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented\n",
      "\n",
      "--- Chunk 2 ---\n",
      "pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic\n",
      "\n",
      "--- Chunk 3 ---\n",
      "the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# In LangChain-Dokumente umwandeln\n",
    "documents = [\n",
    "    Document(page_content=d[\"summary\"], metadata={\"title\": d[\"title\"], \"url\": d[\"url\"]})\n",
    "    for d in docs\n",
    "]\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = ' ',\n",
    "    chunk_size=300, # Fixed Size 100 Zeichen\n",
    "    chunk_overlap=30 # Sliding Window\n",
    "    )\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(chunks)} Chunks erzeugt.\")\n",
    "\n",
    "for i, c in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(c.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dff359",
   "metadata": {},
   "source": [
    "## 3. Embeddings erzeugen und Vektordatenbank erstellen\n",
    "1. WÃ¤hlen Sie ein HuggingFace-Embedding-Modell aus (z.B. eines von denen, die wir uns bereits gemeinsam angeschaut haben). \n",
    "2. Nutzen Sie das Modell, um Embeddings fÃ¼r die Chunks zu erzeugen und eine Vektordatenbank zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c27dacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektordatenbank erstellt.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "db = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=\"../data/arxiv_db1\")\n",
    "\n",
    "print(\"Vektordatenbank erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f536d1",
   "metadata": {},
   "source": [
    "## 4. Semantische Suche in der Vektordatenbank\n",
    "1. Ãœberlegen Sie sich eine Frage oder Phrase, mit der Sie nach Ã¤hnlichen Dokumenten in der Vektordatenbank suchen und schauen Sie sich die Ergebnisse an. \n",
    "2. Experimentieren Sie mit verschiedenen Queries und Werten fÃ¼r `k`. \n",
    "3. Wie kÃ¶nnte man Metadaten (z. B. Jahr, Autor:innen) fÃ¼r die Suche nutzen? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532ae7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Treffer: RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems\n",
      "for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.\n",
      "Quelle: http://arxiv.org/abs/2511.12979v1\n",
      "\n",
      " Treffer: RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems\n",
      "for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.\n",
      "Quelle: http://arxiv.org/abs/2511.12979v1\n",
      "\n",
      " Treffer: SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG\n",
      "capabilities and generative reasoning, can significantly enhance the accuracy and\n",
      " reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,\n",
      " presenting the potential for a new search paradigm capable of providing direct and reliable responses to\n",
      "Quelle: http://arxiv.org/abs/2512.00772v1\n"
     ]
    }
   ],
   "source": [
    "query = \"RAG wie optimieren?\"\n",
    "k = 3\n",
    "# filter nach Autor:in\n",
    "# filter = {\"author\": \"Wang\"}\n",
    "results = db.similarity_search(query, k) # hier mÃ¼sset noch der filter parameter rein\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"\\n Treffer: {hit.metadata['title']}\")\n",
    "    # print(f\"\\n Autor:in: {hit.metadata.get('author' , 'N/A')}\")\n",
    "    print(hit.page_content)\n",
    "    print(\"Quelle:\", hit.metadata[\"url\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75fe02",
   "metadata": {},
   "source": [
    "## 5. Erweiterungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdad527",
   "metadata": {},
   "source": [
    "### 1. Mehr Daten\n",
    "- Holen Sie sich eine grÃ¶ÃŸere Menge an Dokumenten (z.B. `max_results=100`) und erstellen Sie damit eine Vektordatenbank. \n",
    "- Holen Sie sich Artikel zu **verschiedenen** Suchbegriffen und kombinieren Sie diese in einer Vektordatenbank. \n",
    "\n",
    "### 2. Andere Chunking-Methode\n",
    "- Informieren Sie sich Ã¼ber andere MÃ¶glichkeiten des Chunkings mit Hilfe der LangChain-Docs: https://docs.langchain.com/oss/python/integrations/splitters\n",
    "- Probieren Sie mindestens eine **weitere Chunking-Methode** aus und analysieren Sie, was sich verÃ¤ndert. \n",
    "\n",
    "    - `CharacterTextSplitter`: Teilt einen Text nach einer festen Anzahl von Zeichen (chunk_size). Optional kann ein Ãœberlappungsbereich (chunk_overlap) definiert werden\n",
    "\n",
    "    - `RecursiveCharacterTextSplitter`: \n",
    "        - Teilt Text rekursiv, also Schritt fÃ¼r Schritt, von groben zu feineren Trennungen.\n",
    "        - Ziel: Chunks, die ungefÃ¤hr chunk_size Zeichen lang sind, aber SÃ¤tze und AbsÃ¤tze mÃ¶glichst intakt lassen.\n",
    "        - Anders als der einfache CharacterTextSplitter, der stur nach Zeichen splittet, versucht der Recursive Splitter inhaltlich sinnvolle Grenzen zu nutzen.\n",
    "        - `chunk_size` ist notwendig, sonst weiÃŸ der Splitter nicht, wann ein Chunk zu groÃŸ ist.\n",
    "        - Optional: `chunk_overlap` â†’ Ãœberlappung zwischen Chunks; `separators` â†’ Reihenfolge der Trennzeichen (z.â€¯B. [\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "    \n",
    "    - `SentenceTextSplitter`: teilt den Text sauber nach SÃ¤tzen auf.\n",
    "        - Er verwendet typischerweise punktbasierte Regeln (z.â€¯B. . , !, ?) und erkennt Satzgrenzen.\n",
    "        - Ziel: jeder Chunk ist genau ein Satz, optional kann man mehrere SÃ¤tze zu einem Chunk kombinieren, je nach Einstellung.\n",
    "    \n",
    "\n",
    "- **Welche Methode wÃ¼rdem Sie fÃ¼r arXiv-Abstracts empfehlen und warum?**\n",
    "    > - `RecursiveCharacterTextSplitter` oder `SentenceSplitter`\n",
    "    > - GrÃ¼nde: Abstracts haben oft 1â€“3 AbsÃ¤tze\n",
    "    > - SÃ¤tze enthalten komplette Ideen â†’ Embeddings werden sinnvoller\n",
    "    > - Kleine Ãœberlappung (~50 Zeichen) sichert, dass ZusammenhÃ¤nge nicht verloren gehen\n",
    "\n",
    "- WÃ¤re es auch eine MÃ¶glichkeit, gar nicht zu chunken? BegrÃ¼nden Sie.  \n",
    "    - ja bei kÃ¼rzeren Abstracts\n",
    "    - aber je lÃ¤nger um so mehr bÃ¼ÃŸt die Perfromance der semantischen Suche ein:\n",
    "        - Ein Embedding pro Dokument reprÃ¤sentiert den gesamten Inhalt; wichtige Details gehen unter; Treffer werden ungenauer\n",
    "        - Semantische Suche wird schlechter; mehr Themen sind darin vermischt\n",
    "        - Speicherverbrauch steigt\n",
    "\n",
    "\n",
    "### 3. Verfeinerung der Suche\n",
    "- Filtern Sie die Ergebnisse nach Erscheinungsjahr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f2e1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from tqdm import tqdm\n",
    "\n",
    "queries = [\"RAG\", \"Retrieval-Augmented Generation\", \"LLM\", \"Prompt Engineering\"]\n",
    "max_results = 100\n",
    "search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "492a1669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Artikel geladen.\n",
      "\n",
      "ðŸ“„ MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\n",
      "Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a ...\n",
      "\n",
      "ðŸ“„ TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages\n",
      "Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented ...\n"
     ]
    }
   ],
   "source": [
    "client = arxiv.Client()\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append({\n",
    "        \"title\": result.title,\n",
    "        \"summary\": result.summary,\n",
    "        \"url\": result.entry_id,\n",
    "        \"year\": result.published.year\n",
    "    })\n",
    "\n",
    "print(f\"{len(docs)} Artikel geladen.\")\n",
    "for d in docs[:2]:\n",
    "    print(f\"\\nðŸ“„ {d['title']}\\n{d['summary'][:250]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569cd39",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Eine Methode des Chunkings: `RecursiveCharacterTextSplitter`\n",
    "\n",
    "- Bevor der Text einfach nach 300 Zeichen geschnitten wird, versucht der RecursiveCharacterTextSplitter auf Absatz- oder Satzgrenzen zu splitten.\n",
    "- Chunks sind inhaltlich zusammenhÃ¤ngender, weniger abrupt am Wortende.\n",
    "- Weniger Bruchstellen â†’ semantische Embeddings werden oft besser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d3d2828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 Chunks erzeugt.\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented\n",
      "\n",
      "--- Chunk 2 ---\n",
      "retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic\n",
      "\n",
      "--- Chunk 3 ---\n",
      "the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# In LangChain-Dokumente umwandeln\n",
    "documents = [\n",
    "    Document(page_content=d[\"summary\"], metadata={\"title\": d[\"title\"], \"url\": d[\"url\"], \"year\": d[\"year\"]})\n",
    "    for d in docs\n",
    "]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, # Fixed Size 100 Zeichen\n",
    "    chunk_overlap=30, # Sliding Window\n",
    "    )\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(chunks)} Chunks erzeugt.\")\n",
    "\n",
    "for i, c in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(c.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049c408",
   "metadata": {},
   "source": [
    "### Embeddings erzeugen und Vektordatenbank erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "384f084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektordatenbank erstellt.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "db = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=\"../data/arxiv_db_1\")\n",
    "\n",
    "print(\"Vektordatenbank erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eab8d7",
   "metadata": {},
   "source": [
    "### Semantischche Suche\n",
    "\n",
    "- Filtern der Ergebnisse nach Erscheinungsjahr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a131ebcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Treffer: TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction\n",
      "Jahr: 2025\n",
      "Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an\n",
      "Quelle: http://arxiv.org/abs/2511.12520v1\n",
      "\n",
      " Treffer: Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery\n",
      "Jahr: 2025\n",
      "RAG, orchestrated specialized agents and tools to enable discoveries across AI, data science, biomedical, and biosecurity domains. Our system features several specialized agents including the retrieval agent with query planning and response synthesis that enable knowledge retrieval across domains\n",
      "Quelle: http://arxiv.org/abs/2511.18298v1\n",
      "\n",
      " Treffer: Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters\n",
      "Jahr: 2025\n",
      "Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG\n",
      "Quelle: http://arxiv.org/abs/2511.17044v1\n"
     ]
    }
   ],
   "source": [
    "query = \"Definition RAG?\"\n",
    "k = 3\n",
    "filter = {\"year\": {\"$gte\": 2025}}\n",
    "results = db.similarity_search(query, k, filter=filter)\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"\\n Treffer: {hit.metadata['title']}\")\n",
    "    print(f\"Jahr: {hit.metadata.get('year' , 'N/A')}\")\n",
    "    print(hit.page_content)\n",
    "    print(\"Quelle:\", hit.metadata[\"url\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
